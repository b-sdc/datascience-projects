{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from routescience.studylib import datereported, get_mongo\n",
    "from pymongo import MongoClient\n",
    "from bson.raw_bson import RawBSONDocument\n",
    "from pymongoarrow.api import Schema\n",
    "from datetime import datetime\n",
    "from pymongoarrow.monkey import patch_all\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.cm import get_cmap\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from matplotlib.colors import LinearSegmentedColormap, to_hex\n",
    "import geojson\n",
    "import datetime\n",
    "\n",
    "client = MongoClient(f\"mongodb://\")\n",
    "db = client[\"client\"]\n",
    "collection = db[\"collection\"]\n",
    "patch_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions For Graph Types\n",
    "\n",
    "For this section, all you have to do it run the chunk of code. This would be something to copy and paste over to your notebook so that you are able to use these functions. The purpose of this is to keep the notebook looking clean! Another note, these functinos should be usable for all data. For example, there is a bar chart function that is used throughout the collection.\n",
    "\n",
    "Functions included in the code block below are for: Bar charts, line charts, scatterplots, double bar charts, horizontal bar charts, pie charts, vehicle profile charts, and heatmaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pie_chart(data, column=None, title=None, threshold=0.00, width=800, height=800, labels=None):\n",
    "    \"\"\"\n",
    "    Create a pie chart showing the distribution of values in a specified column or variables.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame or list of variables, if DataFrame, it is expected to have a specified column; if list, it contains variables to be plotted\n",
    "    - column: str, the column containing the values to be plotted (only applicable if data is a DataFrame)\n",
    "    - title: str, the title of the chart\n",
    "    - threshold: float, the threshold to filter categories based on their percentages\n",
    "    - width: int, the width of the chart\n",
    "    - height: int, the height of the chart\n",
    "    - labels: list of str, custom labels for the legend (optional)\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.DataFrame) and column:\n",
    "        # If data is a DataFrame and column is specified\n",
    "        value_counts = data[column].value_counts()\n",
    "        category_df = pd.DataFrame({'Category': value_counts.index, 'Count': value_counts.values})\n",
    "        category_df['Percentage'] = category_df['Count'] / category_df['Count'].sum()\n",
    "\n",
    "        # Filter data to only show categories with percentages larger than the threshold\n",
    "        filtered_df = category_df[category_df['Percentage'] >= threshold]\n",
    "\n",
    "        # Create a new DataFrame with custom labels for both legend and pie chart slices\n",
    "        custom_labels_df = pd.DataFrame({'Category': filtered_df['Category'], 'Count': filtered_df['Count']})\n",
    "        custom_labels_df['Label'] = labels[:len(custom_labels_df)] if labels else custom_labels_df['Category']\n",
    "\n",
    "        # Plot the pie chart with custom labels\n",
    "        fig = px.pie(custom_labels_df, values='Count', names='Category', title=title, width=width, height=height)\n",
    "\n",
    "        # Update layout for custom legend labels\n",
    "        if labels:\n",
    "            fig.update_layout(legend_title_text='Legend', legend_tracegroupgap=0)\n",
    "            fig.update_traces(name=labels)\n",
    "\n",
    "        fig.show()\n",
    "    elif isinstance(data, list):\n",
    "        # If data is a list, assume it contains variables to be plotted\n",
    "        total_sum = sum(data)\n",
    "        percentages = [value / total_sum for value in data]\n",
    "\n",
    "        # Create a temporary DataFrame for plotting\n",
    "        temp_df = pd.DataFrame({'Category': range(1, len(data)+1), 'Count': data, 'Percentage': percentages})\n",
    "\n",
    "        # Filter data to only show categories with percentages larger than the threshold\n",
    "        filtered_df = temp_df[temp_df['Percentage'] >= threshold]\n",
    "\n",
    "        # Create a new DataFrame with custom labels for both legend and pie chart slices\n",
    "        custom_labels_df = pd.DataFrame({'Category': filtered_df['Category'], 'Count': filtered_df['Count']})\n",
    "        custom_labels_df['Label'] = labels[:len(custom_labels_df)] if labels else custom_labels_df['Category']\n",
    "\n",
    "        # Plot the pie chart with custom labels\n",
    "        fig = px.pie(custom_labels_df, values='Count', names='Category', title=title, width=width, height=height)\n",
    "\n",
    "        if labels:\n",
    "            fig.update_layout(legend_title_text='Legend', legend_tracegroupgap=0)\n",
    "\n",
    "    # Update the labels attribute with custom labels\n",
    "            fig.data[0].labels = labels\n",
    "\n",
    "        fig.show()\n",
    "    \n",
    "    else:\n",
    "     raise ValueError(\"Invalid input. Provide either a DataFrame with a specified column or a list of variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Event Data Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data_count(data, column):\n",
    "    # break down all legal events and group them by type\n",
    "    columns = data[[\"UniqueTripID\", column]]\n",
    "    normalized_columns = columns.explode(column).dropna()\n",
    "    df = normalized_columns[column].apply(pd.Series)\n",
    "    grouped_columns = columns.join(df).dropna()\n",
    "\n",
    "    grouped_column_count = grouped_columns.groupby('Type').count()\n",
    "    return grouped_column_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Event Data Counts with Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_events_withstops(dataframe, event_column):\n",
    "    \"\"\"\n",
    "    Break down all events and group them by type.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: DataFrame, the input data frame containing the event data\n",
    "    - event_column: str, the name of the column containing the event data to be exploded and grouped\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame, the resulting data frame with grouped events and stops information\n",
    "    \"\"\"\n",
    "    event_col = dataframe[[\"UniqueTripID\", \"Analysis_Stops\", event_column, 'Analysis_AccountID']]\n",
    "    normalized_event_col = event_col.explode(event_column).dropna()\n",
    "    df = normalized_event_col[event_column].apply(pd.Series)\n",
    "    grouped_events = event_col.join(df).dropna()\n",
    "\n",
    "    stops = pd.DataFrame() \n",
    "    stops[['Address', 'City', 'County', 'Jurisdiction']] = grouped_events[\"Analysis_Stops\"].apply(\n",
    "        lambda x: pd.Series([x[0]['Address'], x[0]['City'], x[0]['County'], x[0]['Jurisdiction']]) if len(x) > 0 else pd.Series(['', '', '', ''])\n",
    "    )\n",
    "    grouped_events_with_stops = grouped_events.join(stops).dropna()\n",
    "    \n",
    "    return grouped_events_with_stops\n",
    "\n",
    "# Example usage:\n",
    "# Planned_grouped_closures = gather_events_withstops(analysis_df, \"Analysis_Planned_Safety_Closures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Event Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_event_count(data, column):\n",
    "    columns = data[[\"UniqueTripID\", column]]\n",
    "    normalized_columns = columns.explode(column).dropna()\n",
    "    df = normalized_columns[column].apply(pd.Series)\n",
    "    grouped_columns = columns.join(df).dropna()\n",
    "\n",
    "    grouped_column_count = grouped_columns.groupby('Type')\n",
    "    return grouped_column_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Event Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_event_data(planned_data, actual_data, computed_data):\n",
    "    \"\"\"\n",
    "    Function to merge planned, actual, and computed closure data.\n",
    "    It ensures that missing data is handled by filling NaNs with 0.\n",
    "    \"\"\"\n",
    "    merged_closures = planned_data.merge(actual_data, on='Type', how='outer', suffixes=('_pclose', '_aclose'))\n",
    "    merged_closures = merged_closures.merge(computed_data, on='Type', how='outer', suffixes=('', '_cclose'))\n",
    "\n",
    "    closure_columns = ['DistToStop_pclose', 'DistToStop_aclose', 'DistToStop_cclose']\n",
    "    for col in closure_columns:\n",
    "        if col in merged_closures.columns:\n",
    "            merged_closures[col] = merged_closures[col].fillna(0)\n",
    "    \n",
    "    print(\"Columns after merge:\", merged_closures.columns)  # To inspect column names\n",
    "    \n",
    "    return merged_closures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process RTR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_column_by_type(df, column_name):\n",
    "    col_df = df[[\"UniqueTripID\", column_name]].copy()\n",
    "    normalized_col_df = col_df.explode(column_name).dropna()\n",
    "    expanded_df = normalized_col_df[column_name].apply(pd.Series)\n",
    "    joined_df = normalized_col_df.join(expanded_df).dropna()\n",
    "    grouped_by_class = joined_df.groupby(['UniqueTripID', 'Class']).agg({'DistMiles': 'sum'}).reset_index()\n",
    "    total_distance = grouped_by_class.groupby('UniqueTripID')['DistMiles'].sum().reset_index()\n",
    "    grouped_by_class = grouped_by_class.merge(total_distance, on='UniqueTripID', suffixes=('', '_total'))\n",
    "    grouped_by_class['Percentage'] = (grouped_by_class['DistMiles'] / grouped_by_class['DistMiles_total']) * 100\n",
    "    merged_df = joined_df.merge(grouped_by_class[['UniqueTripID', 'Class', 'Percentage']], on=['UniqueTripID', 'Class'])\n",
    "    cleaned_df = merged_df.drop(column_name, axis=1)\n",
    "    final_grouped_df = expanded_df.groupby('Class').agg({'DistMiles': 'sum', 'DistMilesBuiltUp': 'sum'}).reset_index()\n",
    "\n",
    "    return final_grouped_df\n",
    "\n",
    "# Example usage:\n",
    "# result_df = process_column_by_type(analysis_df, 'Analysis_Actual_RoadTypeReport')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart - Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bar_chart_horizontal(df, y_column, title, n=5, y_label=None, x_label=\"Count\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Create a horizontal bar chart showing the top N values in a specified column.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, the input data containing the values to be plotted\n",
    "    - y_column: str, the column containing the values to be plotted\n",
    "    - title: str, the title of the chart\n",
    "    - n: int, the number of top values to display (default is 5)\n",
    "    - y_label: str, the label for the y-axis (default is None)\n",
    "    - x_label: str, the label for the x-axis (default is \"Count\")\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Group by the specified column and count occurrences, then sort in descending order\n",
    "    column_counts = df[y_column].value_counts().sort_values(ascending=False)\n",
    "\n",
    "    # Select the top N values\n",
    "    top_values = column_counts.head(n)\n",
    "\n",
    "    bars = plt.barh(top_values.index.astype(str), top_values)  # Convert index to strings\n",
    "    \n",
    "    if y_label:\n",
    "        plt.ylabel(y_label)\n",
    "    else:\n",
    "        plt.ylabel(y_column.capitalize())  # Capitalize the column name for better aesthetics\n",
    "    \n",
    "    plt.xlabel(x_label)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Set the zorder of the grid lines to a higher value than the bars\n",
    "    plt.gca().grid(which='both', axis='x', color='gray', linestyle='-', linewidth=0.5, zorder=0)\n",
    "\n",
    "    # Add numbers beside the bars\n",
    "    for bar in bars:\n",
    "        xval = bar.get_width()\n",
    "        plt.text(xval, bar.get_y() + bar.get_height()/2, f\"{int(xval)}\", ha='left', va='center')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_chart(df, type_column, state_column=None, title=None, top_n_states=10, types=None, figure_width=18,\n",
    "                        x_label=\"Count\", y_label=\"States\", orientation='horizontal'):\n",
    "    \"\"\"\n",
    "    Plot a horizontal or vertical countplot of types per state, showing the top N states.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, the input data containing the values to be plotted\n",
    "    - type_column: str, the column representing the event types\n",
    "    - state_column: str or None, the column representing the states (default is None)\n",
    "    - title: str or None, the title of the plot (default is None)\n",
    "    - top_n_states: int, number of top states to display (default is 10)\n",
    "    - types: List of specific 'Type' values to include in the plot (default is None, which includes all).\n",
    "    - figure_width: int, the width of the figure (default is 18)\n",
    "    - x_label: str, the label for the x-axis (default is \"Count\")\n",
    "    - y_label: str, the label for the y-axis (default is \"States\")\n",
    "    - orientation: str, optional, either 'horizontal' (default) or 'vertical'\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(figure_width, 8))\n",
    "\n",
    "    # If types are specified, filter rows accordingly\n",
    "    if types is not None:\n",
    "        if isinstance(types, str):\n",
    "            types = [types]  # Convert a single string to a list\n",
    "        df = df[df[type_column].isin(types)]\n",
    "\n",
    "    if state_column is not None:\n",
    "        # Choose the top N states based on their counts\n",
    "        top_states = df[state_column].value_counts().nlargest(top_n_states).index\n",
    "\n",
    "        # Filter the DataFrame to include only the top N states\n",
    "        df_top_states = df[df[state_column].isin(top_states)]\n",
    "\n",
    "        # Use seaborn's countplot with 'Type' on y-axis and horizontal or vertical orientation\n",
    "        if orientation == 'horizontal':\n",
    "            ax = sns.countplot(y=state_column, hue=type_column, data=df_top_states, orient='h')\n",
    "        elif orientation == 'vertical':\n",
    "            ax = sns.countplot(x=state_column, hue=type_column, data=df_top_states, orient='v')\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for 'orientation'. It should be 'horizontal' or 'vertical'.\")\n",
    "\n",
    "        # Add count values on the right side of the bars\n",
    "        for p in ax.patches:\n",
    "            if orientation == 'horizontal':\n",
    "                ax.annotate(f'{p.get_width()}', (p.get_width(), p.get_y() + p.get_height() / 2.), va='center', ha='left', xytext=(5, 0), textcoords='offset points')\n",
    "            elif orientation == 'vertical':\n",
    "                ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()), va='bottom', ha='center', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "        # Add grid lines behind the bars on the x-axis\n",
    "        if orientation == 'horizontal':\n",
    "            ax.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "        elif orientation == 'vertical':\n",
    "            ax.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.legend(title=type_column, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        # If state_column is None, create a simple countplot without grouping by states\n",
    "        if orientation == 'horizontal':\n",
    "            ax = sns.countplot(y=type_column, data=df, orient='h')\n",
    "        elif orientation == 'vertical':\n",
    "            ax = sns.countplot(x=type_column, data=df, orient='v')\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for 'orientation'. It should be 'horizontal' or 'vertical'.\")\n",
    "\n",
    "        # Add count values on the bars\n",
    "        for p in ax.patches:\n",
    "            if orientation == 'horizontal':\n",
    "                ax.annotate(f'{int(p.get_width())}', (p.get_width(), p.get_y() + p.get_height() / 2.), va='center', ha='left', xytext=(5, 0), textcoords='offset points')\n",
    "            elif orientation == 'vertical':\n",
    "                ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), va='bottom', ha='center', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "        # Add grid lines behind the bars on the x-axis\n",
    "        if orientation == 'horizontal':\n",
    "            ax.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "        elif orientation == 'vertical':\n",
    "            ax.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "        plt.title(title)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel(y_label)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart - Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_bar_chart(merged_event, base_column, legend_labels, title, include_types=None, types=None, figsize=(12, 9), legend_loc='upper right', colors=None):\n",
    "    \"\"\"\n",
    "    Plot a horizontal bar graph for multiple event types from a merged event DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - merged_event: DataFrame with columns for planned, actual, and computed events (or similar).\n",
    "    - event_types: List of event types to include in the plot (e.g., ['pclose', 'aclose', 'cclose']).\n",
    "    - base_column: Base column name prefix (e.g., 'DistToStop', 'Count', etc.).\n",
    "    - legend_labels: List of labels for the legend corresponding to the event_types.\n",
    "    - title: Title of the plot.\n",
    "    - include_types: List of event 'Type' values to include in the plot (default is None, which includes all).\n",
    "    - types: List of specific 'Type' values to include in the plot (default is None, which includes all).\n",
    "    - figsize: Size of the figure.\n",
    "    - legend_loc: Location of the legend in the plot.\n",
    "    - colors: Dictionary mapping event types to colors.\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the plot).\n",
    "    \"\"\"\n",
    "    # If include_types is specified, filter rows accordingly\n",
    "    if include_types is not None:\n",
    "        merged_event = merged_event[merged_event['Type'].isin(include_types)]\n",
    "\n",
    "    # If types is specified, filter rows accordingly\n",
    "    if types is not None:\n",
    "        if isinstance(types, str):\n",
    "            types = [types]  # Convert a single string to a list\n",
    "        merged_event = merged_event[merged_event.index.isin(types)]\n",
    "\n",
    "    # Filter valid columns based on event_types that exist in the DataFrame\n",
    "    valid_event_types = [etype for etype in  ['pclose', 'aclose', 'cclose'] if f'{base_column}_{etype}' in merged_event.columns]\n",
    "    \n",
    "    if not valid_event_types:\n",
    "        print(\"No valid event types found in the DataFrame.\")\n",
    "        return\n",
    "\n",
    "    # Create a 'Total' column by summing the valid event columns\n",
    "    total_column_names = [f'{base_column}_{etype}' for etype in valid_event_types]\n",
    "    merged_event['Total'] = merged_event[total_column_names].sum(axis=1)\n",
    "    merged_event = merged_event.sort_values(by='Total', ascending=True)\n",
    "\n",
    "    # Create a horizontal bar graph with multiple bars for each event type\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    bar_width = 0.2  # Adjust the width as needed\n",
    "\n",
    "    for i, etype in enumerate(valid_event_types):\n",
    "        column_name = f'{base_column}_{etype}'\n",
    "        counts = merged_event[column_name]\n",
    "        positions = range(len(merged_event))\n",
    "\n",
    "        # Adjust the position to place bars next to each other\n",
    "        positions = [pos + i * bar_width for pos in positions]\n",
    "\n",
    "        # Use custom colors if provided, otherwise use a default set of colors\n",
    "        color = colors.get(etype, None) if colors else None\n",
    "\n",
    "        ax.barh(positions, counts, height=bar_width, label=f'{etype.capitalize()}', color=color)\n",
    "\n",
    "        # Annotate bars with count values\n",
    "        for j, count in enumerate(counts):\n",
    "            ax.text(int(count), positions[j], str(int(count)), ha='left', va='center', color='black')\n",
    "\n",
    "    ax.set_xlabel('Count')\n",
    "    ax.set_ylabel('Event Type')\n",
    "    ax.set_yticks(range(len(merged_event)))\n",
    "    ax.set_yticklabels(merged_event.index)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    # Set custom legend labels and title\n",
    "    ax.legend(labels=legend_labels, title='Event Types', loc=legend_loc)\n",
    "\n",
    "    # Add grid lines behind the bars on the x-axis\n",
    "    ax.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart - Individual Route Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_route_event_bar_chart(filtered_merged_closures, suffix, title):\n",
    "    columns_to_include = [col for col in filtered_merged_closures.columns if col.endswith(suffix) or col == 'Total']\n",
    "    \n",
    "    filtered_df = filtered_merged_closures[columns_to_include]\n",
    "    legend_labels = [suffix.replace('_', ' ').title()]\n",
    "\n",
    "    event_bar_chart(\n",
    "        filtered_df,\n",
    "        base_column='DistToStop',\n",
    "        legend_labels=legend_labels,\n",
    "        title=title,\n",
    "        include_types=None,\n",
    "        types=None,\n",
    "        figsize=(12, 9),\n",
    "        legend_loc='lower right',\n",
    "        colors={suffix: 'red'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart - RTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist_miles(dist_miles_actual, dist_miles_planned, dist_miles_computed):\n",
    "    # Create a DataFrame for easier plotting\n",
    "    dist_miles_df = pd.DataFrame({\n",
    "        'Class': dist_miles_actual.index,\n",
    "        'DistMiles_actual': dist_miles_actual.values,\n",
    "        'DistMiles_planned': dist_miles_planned.values,\n",
    "        'DistMiles_computed': dist_miles_computed.values\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    # Plotting the side-by-side bar graph\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Define the width of the bars and the positions\n",
    "    bar_width = 0.25\n",
    "    bar_positions = np.arange(len(dist_miles_df['Class']))\n",
    "\n",
    "    # Plot each bar with the specified colors and positions\n",
    "    bars_actual = ax.barh(bar_positions - bar_width, dist_miles_df['DistMiles_actual'], height=bar_width, color='blue', label='Actual')\n",
    "    bars_planned = ax.barh(bar_positions, dist_miles_df['DistMiles_planned'], height=bar_width, color='red', label='Planned')\n",
    "    bars_computed = ax.barh(bar_positions + bar_width, dist_miles_df['DistMiles_computed'], height=bar_width, color='green', label='Computed')\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_ylabel('Class')\n",
    "    ax.set_xlabel('Total DistMiles')\n",
    "    ax.set_title('Total DistMiles per Class')\n",
    "    ax.set_yticks(bar_positions)\n",
    "    ax.set_yticklabels(dist_miles_df['Class'])\n",
    "    ax.legend()\n",
    "\n",
    "    # Add sum labels to the top of each bar\n",
    "    for bars in [bars_actual, bars_planned, bars_computed]:\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            ax.text(width, bar.get_y() + bar.get_height() / 2, f'{width:.2f}', ha='left', va='center')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_dist_miles(dist_miles_actual, dist_miles_planned, dist_miles_computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(main, *datasets, type_filter=None, custom_title=None, colors=None, dataset_names=None, orientation='horizontal', x_label='Count of Trips', y_label=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot the count of unique trip IDs in the main dataset and additional datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - main: DataFrame or scalar, the main dataset containing unique trip IDs or a scalar value\n",
    "    - *datasets: Variable number of DataFrames, additional datasets to compare with the main dataset\n",
    "    - type_filter: str or None, optional, a filter based on the 'Type' column for the datasets\n",
    "    - custom_title: str or None, optional, a custom title for the plot\n",
    "    - colors: list of str or None, optional, colors for each dataset bar\n",
    "    - dataset_names: list of str or None, optional, names for each dataset\n",
    "    - orientation: str, optional, either 'horizontal' (default) or 'vertical'\n",
    "    - x_label: str, optional, label for the x-axis\n",
    "    - y_label: str or None, optional, label for the y-axis\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the plot)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if main is a DataFrame or scalar\n",
    "    if isinstance(main, pd.DataFrame):\n",
    "        main_count = main['UniqueTripID'].nunique()\n",
    "    elif isinstance(main, (int, float)):\n",
    "        main_count = main\n",
    "    else:\n",
    "        raise ValueError(\"Invalid type for 'main'. It should be a DataFrame or a scalar value.\")\n",
    "\n",
    "    # Filter datasets based on the 'type' column if a type_filter is provided\n",
    "    if type_filter is not None:\n",
    "        datasets = [df[df['Type'] == type_filter] if isinstance(df, pd.DataFrame) else df for df in datasets]\n",
    "\n",
    "    # If main is a scalar, use it directly; otherwise, get the count\n",
    "    main_count = main_count if isinstance(main, (int, float)) else main['UniqueTripID'].nunique()\n",
    "\n",
    "    counts = [df['UniqueTripID'].nunique() if isinstance(df, pd.DataFrame) else df for df in datasets]\n",
    "\n",
    "    # Round the count values to the nearest tenth\n",
    "    main_count = round(main_count, 1)\n",
    "    counts = [round(count, 1) if isinstance(count, (int, float)) else count for count in counts]\n",
    "\n",
    "    # Use default colors and dataset names if not provided\n",
    "    if colors is None:\n",
    "        colors = ['gray', 'red', 'blue', 'green']\n",
    "\n",
    "    if dataset_names is None:\n",
    "        dataset_names = ['Main'] + [f'Dataset {i}' for i in range(1, len(counts) + 1)]\n",
    "\n",
    "    # Create a bar graph with colored bars\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    if orientation == 'horizontal':\n",
    "        bars = ax.barh(dataset_names, [main_count] + counts, color=colors[:len(dataset_names)])\n",
    "        for bar, count in zip(bars, [main_count] + counts):\n",
    "            plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{count}', va='center', ha='left')\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "        plt.xlabel(x_label)\n",
    "        if y_label is not None:\n",
    "            plt.ylabel(y_label)\n",
    "    elif orientation == 'vertical':\n",
    "        bars = ax.bar(dataset_names, [main_count] + counts, color=colors[:len(dataset_names)])\n",
    "        for bar, count in zip(bars, [main_count] + counts):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{count}', va='bottom', ha='center')\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "        plt.ylabel(x_label)\n",
    "        if y_label is not None:\n",
    "            plt.xlabel(y_label)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid value for 'orientation'. It should be 'horizontal' or 'vertical'.\")\n",
    "\n",
    "    # Set the title based on the custom_title parameter\n",
    "    if custom_title is not None:\n",
    "        ax.set_title(custom_title)\n",
    "    else:\n",
    "        default_title = f'Count of Unique IDs in Each Dataset{\" Filtered by Type: \" + type_filter if type_filter else \"\"}'\n",
    "        ax.set_title(default_title)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Bar Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacked_bar_chart(data, type_columns, jurisdiction_column, top_n=5, title='Stacked Bar Chart', x_label=None, y_label=None, type_labels=None, graph_titles=None, cmap='tab10', value_color='black', font_size=8, distance_from_bar=0.02):\n",
    "    \"\"\"\n",
    "    Create a stacked bar chart for multiple type columns, each representing a different category, and stacked by jurisdiction.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame, the input data frame containing information for the chart\n",
    "    - type_columns: list, the list of columns representing different categories for stacking\n",
    "    - jurisdiction_column: str, the column representing jurisdictions for stacking\n",
    "    - top_n: int, the number of top jurisdictions to display (default is 5)\n",
    "    - title: str, the title of the chart (default is 'Stacked Bar Chart')\n",
    "    - x_label: str or None, optional, the label for the x-axis (None if not needed)\n",
    "    - y_label: str or None, optional, the label for the y-axis (None if not needed)\n",
    "    - type_labels: dict or None, optional, a dictionary mapping type column names to custom labels (None if not needed)\n",
    "    - graph_titles: dict or None, optional, a dictionary mapping type column names to custom titles (None if not needed)\n",
    "    - value_color: str, the color of the displayed values on top of each bar (default is 'black')\n",
    "    - font_size: int, the font size for displayed values (default is 8)\n",
    "    - distance_from_bar: float, the distance from the bar for displaying values (default is 0.02)\n",
    "    \"\"\"\n",
    "\n",
    "    # Select the top N jurisdictions based on counts\n",
    "    top_jurisdictions = data[jurisdiction_column].value_counts().nlargest(top_n).index\n",
    "    \n",
    "    # Filter the data for the top jurisdictions\n",
    "    data_top_jurisdictions = data[data[jurisdiction_column].isin(top_jurisdictions)]\n",
    "\n",
    "    # Set default type labels if not specified\n",
    "    if type_labels is None:\n",
    "        type_labels = {col: col.capitalize() for col in type_columns}\n",
    "    \n",
    "    # Set default graph titles if not specified\n",
    "    if graph_titles is None:\n",
    "        graph_titles = {col: f'{title} for {type_labels.get(col, col)}' for col in type_columns}\n",
    "    \n",
    "    # Get a colormap with distinct colors for each type column\n",
    "    color_map = get_cmap(cmap, len(type_columns))\n",
    "\n",
    "    # Create a mapping of type columns to colors\n",
    "    type_color_mapping = {col: color_map(i) for i, col in enumerate(type_columns)}\n",
    "\n",
    "    # Plot a single bar for each jurisdiction with stacking for 'NumRoutes' and 'OORCount'\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Group the data by jurisdiction and calculate counts for each type column\n",
    "    grouped_data = data_top_jurisdictions.groupby(jurisdiction_column)[type_columns].sum()\n",
    "\n",
    "    # Plot the stacked bar chart with distinct colors for each type column\n",
    "    ax = grouped_data.plot(kind='bar', stacked=True, ax=ax, color=[type_color_mapping[col] for col in type_columns])\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(x_label or jurisdiction_column)\n",
    "    ax.set_ylabel(y_label or 'Count')\n",
    "    ax.set_title(title, color='black')\n",
    "\n",
    "    # Show legend with corrected titles\n",
    "    ax.legend(title='Type', labels=[type_labels.get(col, col) for col in type_columns], loc='upper right')\n",
    "\n",
    "    # Add counts on top of each bar (excluding 0 values) to the side of the bars\n",
    "    for idx, p in enumerate(ax.patches):\n",
    "        width, height = p.get_width(), p.get_height()\n",
    "        x, y = p.get_xy()\n",
    "\n",
    "        # Determine the side for annotation based on the index\n",
    "        annotation_side = 'left' if idx % 2 == 0 else 'right'\n",
    "\n",
    "        # Adjust the y-position of annotations to ensure they are within the graph boundaries\n",
    "        annotation_y = y + height / 2\n",
    "\n",
    "        # Adjust the x-position for the side placement\n",
    "        annotation_x = x - distance_from_bar if annotation_side == 'left' else x + width + distance_from_bar\n",
    "\n",
    "        # Add annotation only if the height is greater than 0\n",
    "        if height > 0:\n",
    "            ax.annotate(f'{height}', (annotation_x, annotation_y),\n",
    "                        ha='center', va='center', color=value_color, rotation=0, \n",
    "                        xycoords='data', textcoords='offset points', xytext=(5, 0) if annotation_side == 'left' else (-5, 0),\n",
    "                        fontsize=font_size)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# create_stacked_bar_chart(grouped_data, type_columns=['NumRoutes', 'OORCount'], jurisdiction_column='LaneID', title='Custom Title', font_size=9, distance_from_bar=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Bar/Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacked_bar_chart_table(data, type_columns, jurisdiction_column, top_n=5, title='Stacked Bar Chart', x_label=None, y_label=None, type_labels=None, graph_titles=None, color_gradients=None, value_color='black', font_size=8, distance_from_bar=0.02, x_label_rotation=0, show_counts_threshold=0, table_colors=None, wspace=0.4, table_space=0.8):\n",
    "    # Ensure that type_columns exist in the DataFrame\n",
    "    invalid_columns = set(type_columns) - set(data.columns)\n",
    "    if invalid_columns:\n",
    "        raise ValueError(f\"Invalid columns specified in type_columns: {invalid_columns}\")\n",
    "\n",
    "    # Select the top N jurisdictions based on counts\n",
    "    top_jurisdictions = data[jurisdiction_column].value_counts().nlargest(top_n).index\n",
    "    \n",
    "    # Filter the data for the top jurisdictions\n",
    "    data_top_jurisdictions = data[data[jurisdiction_column].isin(top_jurisdictions)]\n",
    "\n",
    "    # Set default type labels if not specified\n",
    "    if type_labels is None:\n",
    "        type_labels = {col: col.capitalize() for col in type_columns}\n",
    "    \n",
    "    # Set default graph titles if not specified\n",
    "    if graph_titles is None:\n",
    "        graph_titles = {col: f'{title} for {type_labels.get(col, col)}' for col in type_columns}\n",
    "    \n",
    "    # Get a colormap with distinct colors for each unique value within all type columns\n",
    "    default_color_map = plt.get_cmap('tab10', len(data[type_columns].stack().unique()))\n",
    "    \n",
    "    # Create a mapping of type column value to color gradient\n",
    "    if color_gradients is None:\n",
    "        color_gradients = {col: default_color_map for col in type_columns}\n",
    "    else:\n",
    "        for col in type_columns:\n",
    "            if col not in color_gradients:\n",
    "                color_gradients[col] = default_color_map\n",
    "\n",
    "    # Plot a single bar for each jurisdiction with stacking for type columns\n",
    "    fig, (ax, ax_table) = plt.subplots(1, 2, gridspec_kw={'width_ratios': [3, 1]}, figsize=(12, 6))\n",
    "\n",
    "    # Manually adjust the layout to move the table closer to the graph\n",
    "    fig.subplots_adjust(wspace=wspace, right=table_space)\n",
    "\n",
    "    # Plot the stacked bar chart with distinct colors for each value within each type column\n",
    "    bottom_values = None\n",
    "    for col in type_columns:\n",
    "        # Get the color gradient for the current type column\n",
    "        color_gradient = color_gradients[col]\n",
    "\n",
    "        # Iterate through unique values in the current type column\n",
    "        for i, value in enumerate(data_top_jurisdictions[col].unique()):\n",
    "            # Filter the data for the specific type column value\n",
    "            type_data = data_top_jurisdictions[(data_top_jurisdictions[col] == value) & (data_top_jurisdictions[col] != 0)]\n",
    "\n",
    "            # Count occurrences for each jurisdiction\n",
    "            jurisdiction_counts = type_data[jurisdiction_column].value_counts().reindex(top_jurisdictions, fill_value=0)\n",
    "\n",
    "            # Only add to the table if there are non-zero counts\n",
    "            if jurisdiction_counts.sum() > 0:\n",
    "                # Calculate the color based on the color gradient\n",
    "                color = to_hex(color_gradient(i / len(data_top_jurisdictions[col].unique())))\n",
    "\n",
    "                # Plot a bar for each jurisdiction\n",
    "                bars = ax.bar(jurisdiction_counts.index, jurisdiction_counts, label=f'{type_labels.get(col, col)} - {value}', color=color, bottom=bottom_values)\n",
    "\n",
    "                if bottom_values is None:\n",
    "                    bottom_values = jurisdiction_counts\n",
    "                else:\n",
    "                    bottom_values += jurisdiction_counts\n",
    "\n",
    "                # Add counts on top of each bar to the side of the bars\n",
    "                for bar, count in zip(bars, jurisdiction_counts):\n",
    "                    width, height = bar.get_width(), bar.get_height()\n",
    "                    x, y = bar.get_xy()\n",
    "\n",
    "                    # Determine the side for annotation based on the width\n",
    "                    annotation_side = 'left' if width > 0 else 'right'\n",
    "\n",
    "                    # Adjust the y-position of annotations to ensure they are within the graph boundaries\n",
    "                    annotation_y = y + height / 2\n",
    "\n",
    "                    # Adjust the x-position for the side placement\n",
    "                    annotation_x = x - distance_from_bar if annotation_side == 'left' else x + width + distance_from_bar\n",
    "\n",
    "                    # Attempt to move the annotation horizontally to avoid overlap\n",
    "                    x_offset = 10 if annotation_side == 'left' else -10\n",
    "                    annotation_x += x_offset\n",
    "\n",
    "                    # Add annotation only if the height is greater than the threshold\n",
    "                    if height > show_counts_threshold:\n",
    "                        # Add annotation\n",
    "                        ax.annotate(f'{count}', (annotation_x, annotation_y),\n",
    "                                    ha='center', va='center', color=value_color, rotation=0, \n",
    "                                    xycoords='data', textcoords='offset points', xytext=(5, 0) if annotation_side == 'left' else (-5, 0),\n",
    "                                    fontsize=font_size)\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(x_label or jurisdiction_column)\n",
    "    ax.set_ylabel(y_label or 'Count')\n",
    "    ax.set_title(title, color='black')\n",
    "\n",
    "    # Customize x-axis ticks\n",
    "    ax.set_xticks(data_top_jurisdictions[jurisdiction_column].unique())\n",
    "\n",
    "    # Show legend with corrected titles\n",
    "    ax.legend(title='Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # Add a table next to the graph\n",
    "    table_data = []\n",
    "\n",
    "    # Iterate through unique values in the type columns\n",
    "    for col in type_columns:\n",
    "        for value in data_top_jurisdictions[col].unique():\n",
    "            row_data = []\n",
    "            for jurisdiction in top_jurisdictions:\n",
    "                # Filter the data for the specific type column value, jurisdiction, and count occurrences\n",
    "                type_data = data_top_jurisdictions[(data_top_jurisdictions[col] == value) & (data_top_jurisdictions[jurisdiction_column] == jurisdiction)]\n",
    "                count = type_data[col].count()\n",
    "\n",
    "                # Add the count to the row data\n",
    "                row_data.append(count)\n",
    "\n",
    "            # Add the row data to the table data\n",
    "            table_data.append(row_data)\n",
    "\n",
    "    # Use unique values in the type columns as rowLabels\n",
    "    table = ax_table.table(cellText=table_data, rowLabels=[f'{col} - {value}' for col in type_columns for value in data_top_jurisdictions[col].unique()], colLabels=[f'{jurisdiction}' for jurisdiction in top_jurisdictions], loc='center', bbox=[0.1, 0, table_space, 1])\n",
    "\n",
    "    # Add cell colors based on the provided table_colors for table_cells\n",
    "    if table_colors is not None:\n",
    "        for col_idx, col in enumerate(type_columns):\n",
    "            unique_values = data_top_jurisdictions[col].unique()\n",
    "            for j, value in enumerate(unique_values):\n",
    "                for k, jurisdiction in enumerate(top_jurisdictions):\n",
    "                    # Get the color gradient for the current type column\n",
    "                    color_gradient = table_colors[col]\n",
    "\n",
    "                    # Use the proportion of the shifted row index to determine the color\n",
    "                    cell_color = to_hex(color_gradient(j / len(unique_values)))\n",
    "\n",
    "                    # Ensure the index is not negative and is within the range of the table\n",
    "                    row_index = j + len(unique_values) * col_idx + 1\n",
    "                    if 0 <= row_index < len(table.get_celld()):\n",
    "                        table[row_index, k].set_facecolor(cell_color)\n",
    "\n",
    "    # Hide the axes for the table\n",
    "    ax_table.axis('off')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram/Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_boxplot(analysisn, feature, title=None, event_types=None, figsize=(7, 7), kde=False, bins=None, boxplot_color=\"#f1f1f6\", histogram_color=\"royalblue\", threshold=None, legend_position='right'):\n",
    "    \"\"\"\n",
    "    Create a combined histogram and boxplot for a given feature in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - analysisn: DataFrame, the input data frame containing the analysis data\n",
    "    - feature: str, the column name for the feature to be analyzed\n",
    "    - title: str or None, optional, the title of the plot (None for default title)\n",
    "    - event_types: list or None, optional, a list of event types to include in the analysis (None for all event types)\n",
    "    - figsize: tuple, optional, the size of the resulting plot (default is (7, 7))\n",
    "    - boxplot_color: str, optional, the color of the boxplot (default is \"#f1f1f6\")\n",
    "    - histogram_color: str, optional, the color of the histogram (default is \"royalblue\")\n",
    "    - threshold: float or None, optional, a threshold value to filter out data points beyond the threshold (None for no filtering)\n",
    "    - legend_position: str, optional, the position of the legend ('right' or 'left') (default is 'right')\n",
    "    \"\"\"\n",
    "    if event_types:\n",
    "        analysisn = analysisn[analysisn['Type'].isin(event_types)]\n",
    "\n",
    "    if threshold is not None:\n",
    "        # Filter out data points beyond the threshold\n",
    "        analysisn = analysisn[analysisn[feature] <= threshold]\n",
    "\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,\n",
    "        sharex=True,\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )\n",
    "\n",
    "    sns.boxplot(\n",
    "        data=analysisn, x=feature, ax=ax_box2, showmeans=True, color=boxplot_color\n",
    "    ).set(title=f\"Distribution of {title}\" if title else f\"Distribution of {feature}\")\n",
    "\n",
    "    if bins:\n",
    "        sns.histplot(\n",
    "            data=analysisn, x=feature, kde=kde, ax=ax_hist2, bins=bins, color=histogram_color\n",
    "        )\n",
    "    else:\n",
    "        sns.histplot(\n",
    "            data=analysisn, x=feature, kde=kde, ax=ax_hist2, color=histogram_color\n",
    "        )\n",
    "\n",
    "    mean_value = analysisn[feature].mean()\n",
    "    median_value = analysisn[feature].copy().median()\n",
    "    sum_value = analysisn[feature].sum()  # Calculate the sum\n",
    "\n",
    "    ax_hist2.axvline(\n",
    "        mean_value, color=\"green\", linestyle=\"--\", label=f\"Mean ({mean_value:.2f})\"\n",
    "    )\n",
    "    ax_hist2.axvline(\n",
    "        median_value, color=\"black\", linestyle=\"-\", label=f\"Median ({median_value:.2f})\"\n",
    "    )\n",
    "\n",
    "    total_ids = analysisn['UniqueTripID'].nunique()\n",
    "\n",
    "    # Annotate the total IDs right underneath the legend, moved to the right\n",
    "    ax_hist2.text(\n",
    "        1.05,\n",
    "        1.0,\n",
    "        f\"Total IDs: {total_ids}\",\n",
    "        transform=ax_hist2.transAxes,\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        bbox=dict(facecolor='#f1f1f6', edgecolor='gray', boxstyle='round'),\n",
    "    )\n",
    "\n",
    "    # Annotate the sum right underneath the legend, moved to the right\n",
    "    ax_hist2.text(\n",
    "        1.05,\n",
    "        0.90,\n",
    "        f\"Sum: {sum_value:.2f}\",\n",
    "        transform=ax_hist2.transAxes,\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        bbox=dict(facecolor='#f1f1f6', edgecolor='gray', boxstyle='round'),\n",
    "    )\n",
    "    \n",
    "    # Move the entire legend to the left or right without overlapping\n",
    "    if legend_position == 'right':\n",
    "        legend_loc = 'upper left'\n",
    "        bbox_anchor = (1.0, 0.85)  # Adjust the second value here\n",
    "    elif legend_position == 'left':\n",
    "        legend_loc = 'upper right'\n",
    "        bbox_anchor = (-0.3, 1.0)  # Adjust the second value here\n",
    "    else:\n",
    "        raise ValueError(\"Invalid legend position. Use 'left' or 'right'.\")\n",
    "\n",
    "    ax_hist2.legend(loc=legend_loc, bbox_to_anchor=bbox_anchor)\n",
    "\n",
    "# Example usage with a threshold of 100 and legend on the left\n",
    "# histogram_boxplot(analysisn, 'your_feature', threshold=100, legend_position='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Vehicle Combinations and Identify Custom Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_summarize_vehicle_profiles(data_frame):\n",
    "    \"\"\"\n",
    "    Process vehicle combinations in a DataFrame and summarize custom vehicle profiles.\n",
    "\n",
    "    Parameters:\n",
    "    - data_frame: DataFrame, the input data frame containing information about vehicles, including columns 'Height', 'Length', 'Weight', and 'Width'\n",
    "\n",
    "    Returns:\n",
    "    - Tuple, containing two DataFrames:\n",
    "        1. Updated input data frame with an additional column 'combination_name'\n",
    "        2. Summary of custom vehicle combinations with counts, sorted in descending order\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dictionary to map combinations to names\n",
    "    combination_names = {\n",
    "        (16200, 63600, 8000, 10200): 'Sample',\n",
    "        # Add more combinations and names as needed\n",
    "    }\n",
    "\n",
    "    # Create a new column to store the combination name\n",
    "    data_frame['combination_name'] = ''\n",
    "\n",
    "    # Iterate over each row in the dataset\n",
    "    for index, row in data_frame.iterrows():\n",
    "        # Extract the values for length, width, height, and length\n",
    "        height = row['Height']\n",
    "        length = row['Length']\n",
    "        weight = row['Weight']\n",
    "        width = row['Width']\n",
    "\n",
    "        # Define your logic to determine the combination name\n",
    "        combination = (height, length, weight, width)\n",
    "        if combination in combination_names:\n",
    "            combination_name = combination_names[combination]\n",
    "        else:\n",
    "            combination_name = 'custom'\n",
    "\n",
    "        # Assign the combination name to the 'combination_name' column\n",
    "        data_frame.at[index, 'combination_name'] = combination_name\n",
    "\n",
    "    # Filter rows with custom vehicle types\n",
    "    custom_vehicles = data_frame[data_frame['combination_name'] == 'custom']\n",
    "\n",
    "    # Select only the required columns\n",
    "    custom_vehicles = custom_vehicles[['Height', 'Length', 'Width', 'Weight']]\n",
    "\n",
    "    # Get the count of each combination\n",
    "    combinations = custom_vehicles.groupby(['Height', 'Length', 'Width', 'Weight']).size().reset_index(name='count')\n",
    "\n",
    "    # Return the updated DataFrame and the resulting combinations\n",
    "    return data_frame, combinations.sort_values('count', ascending=False)\n",
    "\n",
    "# Example usage:\n",
    "# updated_data_frame, custom_combinations_summary = process_and_summarize_vehicle_profiles(original_data_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(df, columns=None, title=\"Heatmap\", x_title=None, y_title=None):\n",
    "    \"\"\"\n",
    "    Create a heatmap from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, the data frame containing the data\n",
    "    - columns: list or None, optional, a list of column names to be used for the heatmap; if None, all columns will be used\n",
    "    - title: str, the title of the heatmap (default is \"Heatmap\")\n",
    "    - x_title: str or None, optional, the title for the x-axis\n",
    "    - y_title: str or None, optional, the title for the y-axis\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # If specific columns are provided, select only those columns\n",
    "    if columns is not None:\n",
    "        df = df[columns]\n",
    "\n",
    "    # Convert dictionaries to strings\n",
    "    df = df.applymap(lambda x: str(x) if isinstance(x, dict) else x)\n",
    "\n",
    "    sns.heatmap(df.apply(pd.value_counts).fillna(0), annot=True, fmt='g', cmap='Blues')\n",
    "\n",
    "    # Set x and y titles if provided\n",
    "    if x_title is not None:\n",
    "        plt.xlabel(x_title)\n",
    "    if y_title is not None:\n",
    "        plt.ylabel(y_title)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(c_df_filtered, column):\n",
    "    \"\"\"\n",
    "    Remove outliers from a DataFrame based on a specified column using the IQR method.\n",
    "\n",
    "    Parameters:\n",
    "    - c_df_filtered: DataFrame, the input data frame containing the column with potential outliers\n",
    "    - column: str, the name of the column to check for outliers\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame, a filtered DataFrame excluding the outliers\n",
    "    \"\"\"\n",
    "\n",
    "    Q1 = c_df_filtered[column].quantile(0.25)\n",
    "    Q3 = c_df_filtered[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # filter outliers\n",
    "    df_filtered = c_df_filtered[(c_df_filtered[column] >= Q1 - 1.5 * IQR) & (c_df_filtered[column] <= Q3 + 1.5 * IQR)]\n",
    "\n",
    "    return(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Average Distance To Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_dist_to_stop(dataset1, dataset2, dataset3, num_states=None, title=None, type=None):\n",
    "    \"\"\"\n",
    "    Plot the average 'DistToStop' for the top states across three datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset1, dataset2, dataset3: DataFrames, the three datasets to be compared\n",
    "    - num_states: int or None, the number of top states to include in the plot\n",
    "    - title: str or None, optional, the title of the plot\n",
    "    - type: str or None, optional, a specific type to filter the data (e.g., 'Planned', 'Actual', 'Computed')\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the plot)\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine all datasets\n",
    "    all_datasets = pd.concat([dataset1, dataset2, dataset3], ignore_index=True)\n",
    "\n",
    "    # Filter the data based on the specified type variable\n",
    "    if type:\n",
    "        all_datasets = all_datasets[all_datasets['Type'] == type]\n",
    "\n",
    "    # Calculate the average 'DistToStop' for each state across all datasets\n",
    "    avg_dist_to_stop_all = all_datasets.groupby('Jurisdiction')['DistToStop'].mean()\n",
    "\n",
    "    # Get the top num_states states based on the highest means across all datasets\n",
    "    top_states = avg_dist_to_stop_all.nlargest(num_states).index\n",
    "\n",
    "    # Calculate the average 'DistToStop' for each dataset and state\n",
    "    avg_dist_to_stop_1 = dataset1.groupby('Jurisdiction')['DistToStop'].mean()\n",
    "    avg_dist_to_stop_2 = dataset2.groupby('Jurisdiction')['DistToStop'].mean()\n",
    "    avg_dist_to_stop_3 = dataset3.groupby('Jurisdiction')['DistToStop'].mean()\n",
    "\n",
    "    # Combine the data for plotting\n",
    "    states_union = avg_dist_to_stop_1.index.union(avg_dist_to_stop_2.index).union(avg_dist_to_stop_3.index).intersection(top_states)\n",
    "\n",
    "    values_1 = [avg_dist_to_stop_1.get(key, 0) for key in states_union]\n",
    "    values_2 = [avg_dist_to_stop_2.get(key, 0) for key in states_union]\n",
    "    values_3 = [avg_dist_to_stop_3.get(key, 0) for key in states_union]\n",
    "\n",
    "     # Set the figure size\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    # Create a bar graph with adjusted bar width and spacing\n",
    "    bar_width = 0.15\n",
    "    space_width = 0.1\n",
    "    bar_positions_1 = range(len(states_union))\n",
    "    bar_positions_2 = [pos + bar_width + space_width for pos in bar_positions_1]\n",
    "    bar_positions_3 = [pos + 2*(bar_width + space_width) for pos in bar_positions_1]\n",
    "\n",
    "    plt.bar(bar_positions_1, values_1, width=bar_width, label='Planned', color='red')\n",
    "    plt.bar(bar_positions_2, values_2, width=bar_width, label='Actual', color='blue')\n",
    "    plt.bar(bar_positions_3, values_3, width=bar_width, label='Computed', color='green')\n",
    "\n",
    "    # Display values on top of bars\n",
    "    for pos, value in zip(bar_positions_1, values_1):\n",
    "        plt.text(pos + bar_width / 2 - 0.1, value, f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "    for pos, value in zip(bar_positions_2, values_2):\n",
    "        plt.text(pos + bar_width / 2 - 0.1, value, f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "    for pos, value in zip(bar_positions_3, values_3):\n",
    "        plt.text(pos + bar_width / 2 - 0.1, value, f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.xlabel('State')\n",
    "    plt.ylabel(f'Average DistToStop')\n",
    "    plt.title(title if title else f'Average DistToStop for Top {num_states} States ({type} Type)')\n",
    "    plt.xticks([pos + (bar_width + space_width) for pos in bar_positions_1], states_union)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
