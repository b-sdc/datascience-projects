{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson.raw_bson import RawBSONDocument\n",
    "from pymongoarrow.api import Schema\n",
    "from datetime import datetime\n",
    "from pymongoarrow.monkey import patch_all\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from matplotlib.colors import LinearSegmentedColormap, to_hex\n",
    "import geojson\n",
    "import datetime\n",
    "\n",
    "mongo_connection = f\"mongodb://\"\n",
    "database = \"Impact_Analysis\"\n",
    "baseline_collection = \"baseline\" # insert Impact Analysis MASTER collection name here\n",
    "dev_collection = \"dev\" # insert Impact Analysis DEV collection name here\n",
    "\n",
    "baseline_color = 'darkblue'\n",
    "baseline_label = 'Baseline'\n",
    "\n",
    "dev_color = '#1f77b4'\n",
    "dev_label = 'Dev'\n",
    "\n",
    "custom_colors = [baseline_color, dev_color]\n",
    "custom_labels = [baseline_label, dev_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Graph Types\n",
    "\n",
    "For this section, all you have to do it run the chunk of code. This would be something to copy and paste over to your notebook so that you are able to use these functions. The purpose of this is to keep the notebook looking clean! Another note, these functinos should be usable for all data. For example, there is a bar chart function that is used throughout the collection.\n",
    "\n",
    "Functions included in the code block below are for: Bar charts, line charts, scatterplots, double bar charts, horizontal bar charts, pie charts, vehicle profile charts, and heatmaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_datasets(main, *datasets, type_filter=None, custom_title=None, colors=None, dataset_names=None, orientation='horizontal', x_label='Count of Trips', y_label=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot the count of unique trip IDs in the main dataset and additional datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - main: DataFrame or scalar, the main dataset containing unique trip IDs or a scalar value\n",
    "    - *datasets: Variable number of DataFrames, additional datasets to compare with the main dataset\n",
    "    - type_filter: str or None, optional, a filter based on the 'Type' column for the datasets\n",
    "    - custom_title: str or None, optional, a custom title for the plot\n",
    "    - colors: list of str or None, optional, colors for each dataset bar\n",
    "    - dataset_names: list of str or None, optional, names for each dataset\n",
    "    - orientation: str, optional, either 'horizontal' (default) or 'vertical'\n",
    "    - x_label: str, optional, label for the x-axis\n",
    "    - y_label: str or None, optional, label for the y-axis\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the plot)\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(main, pd.DataFrame):\n",
    "        main_count = main['UniqueTripID'].nunique()\n",
    "    elif isinstance(main, (int, float)):\n",
    "        main_count = main\n",
    "    else:\n",
    "        raise ValueError(\"Invalid type for 'main'. It should be a DataFrame or a scalar value.\")\n",
    "\n",
    "    if type_filter is not None:\n",
    "        datasets = [df[df['Type'] == type_filter] if isinstance(df, pd.DataFrame) else df for df in datasets]\n",
    "\n",
    "    main_count = main_count if isinstance(main, (int, float)) else main['UniqueTripID'].nunique()\n",
    "\n",
    "    counts = [df['UniqueTripID'].nunique() if isinstance(df, pd.DataFrame) else df for df in datasets]\n",
    "\n",
    "    main_count = round(main_count, 1)\n",
    "    counts = [round(count, 1) if isinstance(count, (int, float)) else count for count in counts]\n",
    "\n",
    "    if colors is None:\n",
    "        colors = ['gray', 'red', 'blue', 'green']\n",
    "\n",
    "    if dataset_names is None:\n",
    "        dataset_names = ['Main'] + [f'Dataset {i}' for i in range(1, len(counts) + 1)]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    if orientation == 'horizontal':\n",
    "        bars = ax.barh(dataset_names, [main_count] + counts, color=colors[:len(dataset_names)])\n",
    "        for bar, count in zip(bars, [main_count] + counts):\n",
    "            plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{count}', va='center', ha='left')\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "        plt.xlabel(x_label)\n",
    "        if y_label is not None:\n",
    "            plt.ylabel(y_label)\n",
    "    elif orientation == 'vertical':\n",
    "        bars = ax.bar(dataset_names, [main_count] + counts, color=colors[:len(dataset_names)])\n",
    "        for bar, count in zip(bars, [main_count] + counts):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{count}', va='bottom', ha='center')\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "        plt.ylabel(x_label)\n",
    "        if y_label is not None:\n",
    "            plt.xlabel(y_label)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid value for 'orientation'. It should be 'horizontal' or 'vertical'.\")\n",
    "\n",
    "    if custom_title is not None:\n",
    "        ax.set_title(custom_title)\n",
    "    else:\n",
    "        default_title = f'Count of Unique IDs in Each Dataset{\" Filtered by Type: \" + type_filter if type_filter else \"\"}'\n",
    "        ax.set_title(default_title)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram/Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_boxplot(df, feature, title=None, event_types=None, figsize=(7, 7), kde=False, bins=None, boxplot_color=\"#f1f1f6\", histogram_color=\"royalblue\", threshold=None, legend_position='right'):\n",
    "    \"\"\"\n",
    "    Create a combined histogram and boxplot for a given feature in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, the input data frame containing the analysis data\n",
    "    - feature: str, the column name for the feature to be analyzed\n",
    "    - title: str or None, optional, the title of the plot (None for default title)\n",
    "    - event_types: list or None, optional, a list of event types to include in the analysis (None for all event types)\n",
    "    - figsize: tuple, optional, the size of the resulting plot (default is (7, 7))\n",
    "    - boxplot_color: str, optional, the color of the boxplot (default is \"#f1f1f6\")\n",
    "    - histogram_color: str, optional, the color of the histogram (default is \"royalblue\")\n",
    "    - threshold: float or None, optional, a threshold value to filter out data points beyond the threshold (None for no filtering)\n",
    "    - legend_position: str, optional, the position of the legend ('right' or 'left') (default is 'right')\n",
    "    \"\"\"\n",
    "    if event_types:\n",
    "        df = df[df['Type'].isin(event_types)]\n",
    "\n",
    "    if threshold is not None:\n",
    "        df = df[df[feature] <= threshold]\n",
    "\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,\n",
    "        sharex=True,\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )\n",
    "\n",
    "    sns.boxplot(\n",
    "        data=df, x=feature, ax=ax_box2, showmeans=True, color=boxplot_color\n",
    "    ).set(title=f\"Distribution of {title}\" if title else f\"Distribution of {feature}\")\n",
    "\n",
    "    if bins:\n",
    "        sns.histplot(\n",
    "            data=df, x=feature, kde=kde, ax=ax_hist2, bins=bins, color=histogram_color\n",
    "        )\n",
    "    else:\n",
    "        sns.histplot(\n",
    "            data=df, x=feature, kde=kde, ax=ax_hist2, color=histogram_color\n",
    "        )\n",
    "\n",
    "    mean_value = df[feature].mean()\n",
    "    median_value = df[feature].copy().median()\n",
    "    sum_value = df[feature].sum() \n",
    "\n",
    "    ax_hist2.axvline(\n",
    "        mean_value, color=\"green\", linestyle=\"--\", label=f\"Mean ({mean_value:.2f})\"\n",
    "    )\n",
    "    ax_hist2.axvline(\n",
    "        median_value, color=\"black\", linestyle=\"-\", label=f\"Median ({median_value:.2f})\"\n",
    "    )\n",
    "\n",
    "    total_ids = df['UniqueID'].nunique()\n",
    "\n",
    "    ax_hist2.text(\n",
    "        1.05,\n",
    "        1.0,\n",
    "        f\"Total IDs: {total_ids}\",\n",
    "        transform=ax_hist2.transAxes,\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        bbox=dict(facecolor='#f1f1f6', edgecolor='gray', boxstyle='round'),\n",
    "    )\n",
    "\n",
    "    ax_hist2.text(\n",
    "        1.05,\n",
    "        0.90,\n",
    "        f\"Sum: {sum_value:.2f}\",\n",
    "        transform=ax_hist2.transAxes,\n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='top',\n",
    "        bbox=dict(facecolor='#f1f1f6', edgecolor='gray', boxstyle='round'),\n",
    "    )\n",
    "    \n",
    "    if legend_position == 'right':\n",
    "        legend_loc = 'upper left'\n",
    "        bbox_anchor = (1.0, 0.85)  \n",
    "    elif legend_position == 'left':\n",
    "        legend_loc = 'upper right'\n",
    "        bbox_anchor = (-0.3, 1.0)  \n",
    "    else:\n",
    "        raise ValueError(\"Invalid legend position. Use 'left' or 'right'.\")\n",
    "\n",
    "    ax_hist2.legend(loc=legend_loc, bbox_to_anchor=bbox_anchor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, column):\n",
    "    \"\"\"\n",
    "    Remove outliers from a DataFrame based on a specified column using the IQR method.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame, the input data frame containing the column with potential outliers\n",
    "    - column: str, the name of the column to check for outliers\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame, a filtered DataFrame excluding the outliers\n",
    "    \"\"\"\n",
    "\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    df_filtered = df[(df[column] >= Q1 - 1.5 * IQR) & (df[column] <= Q3 + 1.5 * IQR)]\n",
    "\n",
    "    return(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data_count(df, column):\n",
    "    # break down all events and group by type\n",
    "    columns = df[[\"UniqueID\", column]]\n",
    "    normalized_columns = columns.explode(column).dropna()\n",
    "    df = normalized_columns[column].apply(pd.Series)\n",
    "    grouped_columns = columns.join(df).dropna()\n",
    "\n",
    "    grouped_column_count = grouped_columns.groupby('Type').count()\n",
    "    return grouped_column_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Event Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_event_data(merge_column, event_column, baseline, dev):\n",
    "    merged_data = baseline.merge(dev, on=merge_column, how='outer', suffixes=('_base', '_dev'))\n",
    "    merged_data[[f'{event_column}_base', f'{event_column}_dev']] = merged_data[[f'{event_column}_base', f'{event_column}_dev']].fillna(0)\n",
    "\n",
    "    return merged_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart - Events/RTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_bar_chart(df, merge_column, event_column, event_types, legend_labels, title, x_label, y_label, include_types=None, types=None, figsize=(12, 9), legend_loc='upper right', colors=None):\n",
    "    \"\"\"\n",
    "    Plot a horizontal bar graph for multiple event types.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Merged and filtered DataFrame with event type columns.\n",
    "    - event_types: List of event types to include in the plot.\n",
    "    - include_types: List of event types to include in the plot (default is None, which includes all).\n",
    "    - types: List of specific 'Type' values to include in the plot (default is None, which includes all).\n",
    "    - figsize: Size of the figure.\n",
    "    - legend_loc: Location of the legend in the plot.\n",
    "    - colors: Dictionary mapping event types to colors.\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the plot).\n",
    "    \"\"\"\n",
    "    # If 'Type' is not in the columns, reset the index to make it a column\n",
    "    if merge_column not in df.columns:\n",
    "        df = df.reset_index()\n",
    "\n",
    "    event_columns = [f'{event_column}_{etype}' for etype in event_types]\n",
    "\n",
    "    if include_types is not None:\n",
    "        df = df[df[merge_column].isin(include_types)]\n",
    "\n",
    "    if types is not None:\n",
    "        if isinstance(types, str):\n",
    "            types = [types] \n",
    "        df = df[df[merge_column].isin(types)]\n",
    "\n",
    "    valid_event_types = [etype for etype in event_types if f'{event_column}_{etype}' in df.columns]\n",
    "    if not valid_event_types:\n",
    "        print(\"No valid event types found in the DataFrame.\")\n",
    "        return\n",
    "\n",
    "    total_column = f'Total_{\", \".join(valid_event_types)}'\n",
    "    df[total_column] = df[event_columns].sum(axis=1)\n",
    "    df = df.sort_values(by=total_column, ascending=True)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    bar_width = 0.2 \n",
    "\n",
    "    for i, etype in enumerate(valid_event_types):\n",
    "        column_name = f'{event_column}_{etype}'\n",
    "        counts = df[column_name]\n",
    "        positions = range(len(df))\n",
    "\n",
    "        positions = [pos - i * bar_width for pos in positions]\n",
    "        color = colors.get(etype, None) if colors else None\n",
    "        ax.barh(positions, counts, height=bar_width, label=f'{etype.capitalize()}', color=color)\n",
    "        for j, count in enumerate(counts):\n",
    "            ax.text(int(count), positions[j], str(int(count)), ha='left', va='center', color='black')\n",
    "\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_yticks(range(len(df)))\n",
    "    ax.set_yticklabels(df[merge_column])\n",
    "    ax.set_title(title)\n",
    "\n",
    "    legend_labels = legend_labels \n",
    "    ax.legend(labels=legend_labels, loc=legend_loc)\n",
    "    ax.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge RTR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_road_type_data(df):\n",
    "    crtr = df[[\"UniqueTripID\", \"Analysis_Computed_RoadTypeReport\"]]\n",
    "    cnormalize_col = crtr.explode(\"Analysis_Computed_RoadTypeReport\").dropna()\n",
    "    adf = cnormalize_col['Analysis_Computed_RoadTypeReport'].apply(pd.Series)\n",
    "    agrouped = crtr.join(adf).dropna()\n",
    "\n",
    "    agrouped_by_class = agrouped.groupby(['UniqueTripID', 'Class']).agg({'DistMiles': 'sum'}).reset_index()\n",
    "\n",
    "    total_distance = agrouped_by_class.groupby('UniqueTripID')['DistMiles'].sum().reset_index()\n",
    "    agrouped_by_class = agrouped_by_class.merge(total_distance, on='UniqueTripID', suffixes=('', '_total'))\n",
    "    agrouped_by_class['Percentage'] = (agrouped_by_class['DistMiles'] / agrouped_by_class['DistMiles_total']) * 100\n",
    "    rtr_merged_df = agrouped.merge(agrouped_by_class[['UniqueTripID', 'Class', 'Percentage']], on=['UniqueTripID', 'Class'])\n",
    "    rtr_merged_df = rtr_merged_df.drop('Analysis_Computed_RoadTypeReport', axis=1)\n",
    "    artr_grouped_df = adf.groupby('Class').agg({'DistMiles': 'sum', 'DistMilesBuiltUp': 'sum'})\n",
    "    artr_grouped_df = artr_grouped_df.reset_index()\n",
    "    \n",
    "    return artr_grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Schema & Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_schema = {\n",
    "\n",
    " }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_projection = {\n",
    "\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account Name Schema & Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_schema = {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_projection = {\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Impact Results - without change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(mongo_connection)\n",
    "db1 = client[database]\n",
    "collection1 = db1[baseline_collection] \n",
    "patch_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = collection1.aggregate_pandas_all([\n",
    "    {\"$project\" : default_projection}\n",
    "    ],\n",
    "    schema = Schema(default_schema)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev Impact Result - with change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(mongo_connection)\n",
    "db2 = client[database]\n",
    "collection2 = db2[dev_collection] \n",
    "patch_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = collection2.aggregate_pandas_all([\n",
    "    {\"$sample\" : {\"size\" : 10000}},\n",
    "    {\"$project\" : default_projection}\n",
    "    ],\n",
    "    schema = Schema(default_schema)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient(mongo_connection)\n",
    "dbaccount = client[\"Accounts\"]\n",
    "accountcollection = dbaccount[\"accounts_map\"]\n",
    "patch_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accountname = accountcollection.aggregate_pandas_all([\n",
    "    {\"$project\" : account_projection}\n",
    "    ],\n",
    "    schema = Schema(account_schema)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_master = pd.DataFrame(master)\n",
    "impact_dev = pd.DataFrame(dev)\n",
    "accountname = pd.DataFrame(accountname)\n",
    "\n",
    "accountname['AccountID'] = accountname['AccountID'].astype(int)\n",
    "\n",
    "impact_master = impact_master.merge(accountname, left_on = \"AccountID\", right_on = \"AccountID\", how = \"left\")\n",
    "impact_dev = impact_dev.merge(accountname, left_on = \"AccountID\", right_on = \"AccountID\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_master = impact_master.drop_duplicates(subset=['UniqueID'])\n",
    "impact_dev = impact_dev.drop_duplicates(subset=['UniqueID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_ids = set(impact_master['UniqueID']).intersection(impact_dev['UniqueID'])\n",
    "impact_master = impact_master[impact_master['UniqueID'].isin(common_ids)]\n",
    "impact_dev = impact_dev[impact_dev['UniqueID'].isin(common_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(impact_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(impact_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance (Miles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Sum of Dist Miles - Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cSum_dev = impact_dev['Distance'].sum()\n",
    "cSum = impact_master['Distance'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_datasets(cSum, cSum_dev, custom_title='Sum of Mileages', colors=custom_colors, dataset_names=custom_labels, orientation='vertical',  x_label='Count of Miles', y_label='Branch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Mean of Distance - Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cMean_dev = impact_dev['Distance'].mean()\n",
    "cMean = impact_master['Distance'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_datasets(cMean, cMean_dev, custom_title='Average of Mileages', colors=custom_colors, dataset_names=custom_labels, orientation='vertical',  x_label='Average of Miles', y_label='Branch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mileage Diff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (EDIT) Create .csv to Investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mileage_master = impact_master[['UniqueID','DistMiles']]\n",
    "mileage_dev = impact_dev[['UniqueTripID','DistMiles']]\n",
    "mileage_combined = pd.merge(mileage_master, mileage_dev, on='UniqueID', suffixes=('_master', '_dev'))\n",
    "mileage_combined['Mileage_diff'] = abs(mileage_combined['DistMiles'] - mileage_combined['DistMiles'])\n",
    "mileage_combined = mileage_combined.sort_values(by='Mileage_diff', ascending=False)\n",
    "\n",
    "# mileage_combined.to_csv('all.csv', index=False) # Edit name of file to be output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot Histogram for Mileage Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(mileage_combined, 'Mileage_diff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimated Time Taken (ETT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert ETTSeconds to HH:MM:SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seconds(seconds):\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return hours, minutes, seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Sum of ETT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cETTSum_dev = impact_dev['Analysis_Computed_ETTSeconds'].sum()\n",
    "cETTSum = impact_master['Analysis_Computed_ETTSeconds'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_datasets(cETTSum, cETTSum_dev, custom_title='Sum of ETTSeconds', colors=custom_colors, dataset_names=custom_labels, orientation='vertical',  x_label='ETT', y_label='Branch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Average of ETT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cETTMean_dev = impact_dev['ETTS'].mean()\n",
    "cETTMean = impact_master['ETTS'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_datasets(cETTMean, cETTMean_dev, custom_title='Average of ETTSeconds', colors=custom_colors, dataset_names=custom_labels, orientation='vertical',  x_label='ETT', y_label='Branch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_legal_type_counts = gather_data_count(impact_master, 'Legal')\n",
    "dev_legal_type_counts = gather_data_count(impact_dev, 'Legal')\n",
    "\n",
    "merged_legal = merge_event_data('Type', 'Legal', baseline_legal_type_counts, dev_legal_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_bar_chart(merged_legal, 'Type', 'Legal', event_types=['base', 'dev'], legend_labels = ['Baseline Legal', 'Dev Legal'], title = 'Counts of Legal Events', x_label='Count', y_label='Event Type', colors={'base': baseline_color, 'dev': dev_color}, legend_loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Type Report (RTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtr_baseline = gather_road_type_data(impact_master)\n",
    "rtr_dev = gather_road_type_data(impact_dev)\n",
    "\n",
    "rtr_merged = merge_event_data('Class', 'DistMiles', rtr_baseline, rtr_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_bar_chart(rtr_merged, 'Class', 'DistMiles', event_types=['base', 'dev'], legend_labels = ['Baseline', 'Dev'], title = 'Total Mileage by Road Class', x_label='Mileage', y_label='Road Class', colors={'base': baseline_color, 'dev': dev_color}, legend_loc='lower right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (v3.8.3:6f8c8320e9, May 13 2020, 16:29:34) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
